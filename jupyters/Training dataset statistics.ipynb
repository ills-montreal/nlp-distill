{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "paper_path = Path(\"../../papers/Distillation-MI-ICLR/tables/nlp/\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def load_specter():\n",
    "    \"\"\"\n",
    "    embedding-data/SPECTER\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"embedding-data/specter\")\n",
    "    dataset = dataset[\"train\"]\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for elem in dataset:\n",
    "        s1, s2 = elem[\"set\"][0], elem[\"set\"][1]\n",
    "        samples.append(s1)\n",
    "        samples.append(s2)\n",
    "\n",
    "    df = pd.DataFrame(samples, columns=[\"text\"])\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_amazon_qa():\n",
    "    dataset = load_dataset(\"embedding-data/Amazon-QA\")\n",
    "    dataset = dataset[\"train\"]\n",
    "\n",
    "    samples = []\n",
    "    for elem in dataset:\n",
    "        query = elem[\"query\"]\n",
    "        samples.append(query)\n",
    "        for answer in elem[\"pos\"]:\n",
    "            samples.append(answer)\n",
    "\n",
    "    df = pd.DataFrame(samples, columns=[\"text\"])\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_simple_wiki():\n",
    "    dataset = load_dataset(\"embedding-data/simple-wiki\")\n",
    "    dataset = dataset[\"train\"]\n",
    "\n",
    "    samples = []\n",
    "    for elem in dataset:\n",
    "        for s in elem[\"set\"]:\n",
    "            samples.append(s)\n",
    "\n",
    "    df = pd.DataFrame(samples, columns=[\"text\"])\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_QQP_triplets():\n",
    "    dataset = load_dataset(\"embedding-data/QQP_triplets\")\n",
    "    dataset = dataset[\"train\"]\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for elem in dataset:\n",
    "        d = elem[\"set\"]\n",
    "        samples.append(d[\"query\"])\n",
    "\n",
    "        for s in d[\"pos\"]:\n",
    "            samples.append(s)\n",
    "\n",
    "        for s in d[\"neg\"]:\n",
    "            samples.append(s)\n",
    "\n",
    "    df = pd.DataFrame(samples, columns=[\"text\"])\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_sentence_compression():\n",
    "    dataset = load_dataset(\"embedding-data/sentence-compression\")\n",
    "    dataset = dataset[\"train\"]\n",
    "\n",
    "    samples = []\n",
    "    for elem in dataset:\n",
    "        for s in elem[\"set\"]:\n",
    "            samples.append(s)\n",
    "\n",
    "    df = pd.DataFrame(samples, columns=[\"text\"])\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_altlex():\n",
    "    dataset = load_dataset(\"embedding-data/altlex\")\n",
    "    dataset = dataset[\"train\"]\n",
    "\n",
    "    samples = []\n",
    "    for elem in dataset:\n",
    "        for s in elem[\"set\"]:\n",
    "            samples.append(s)\n",
    "\n",
    "    df = pd.DataFrame(samples, columns=[\"text\"])\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_agnews():\n",
    "    dataset = load_dataset(\"fancyzhx/ag_news\")\n",
    "    dataset = dataset[\"train\"]\n",
    "\n",
    "    df = dataset.to_pandas()\n",
    "    df = df[\"text\"].to_frame()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_sst2():\n",
    "    dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "    dataset = dataset[\"train\"]\n",
    "    df = dataset.to_pandas()[\"sentence\"].to_frame()\n",
    "    df = df.rename(columns={\"sentence\": \"text\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_dair_emotion():\n",
    "    dataset = load_dataset(\"dair-ai/emotion\", \"unsplit\")\n",
    "    dataset = dataset[\"train\"]\n",
    "    df = dataset.to_pandas()\n",
    "    df = df[\"text\"].to_frame()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_snli():\n",
    "    dataset = load_dataset(\"stanfordnlp/snli\")\n",
    "\n",
    "    dataset = dataset[\"train\"]\n",
    "\n",
    "    premise, hypothesis = (dataset[\"premise\"], dataset[\"hypothesis\"])\n",
    "    df_premise = pd.DataFrame(premise, columns=[\"text\"])\n",
    "    df_hypothesis = pd.DataFrame(hypothesis, columns=[\"text\"])\n",
    "\n",
    "    df = pd.concat([df_premise, df_hypothesis])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def tweet_eval():\n",
    "    dataset = load_dataset(\"cardiffnlp/tweet_eval\", \"emoji\")\n",
    "\n",
    "    dataset = dataset[\"train\"]\n",
    "\n",
    "    df = dataset.to_pandas()\n",
    "    df = df[\"text\"].to_frame()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_imdb():\n",
    "    dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "    dataset = dataset[\"train\"]\n",
    "\n",
    "    df = dataset.to_pandas()\n",
    "    df = df[\"text\"].to_frame()\n",
    "\n",
    "    return df\n"
   ],
   "id": "3c19d055538a4cbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Load all datasets and compute statistics\n",
    "\n",
    "\n",
    "# load dataset statistics\n",
    "datasets = {\n",
    "    \"SPECTER\": (load_specter(), \"embedding-data/SPECTER\"),\n",
    "    \"Amazon-QA\": (load_amazon_qa(), \"embedding-data/Amazon-QA\"),\n",
    "    \"Simple-wiki\": (load_simple_wiki(), \"embedding-data/simple-wiki\"),\n",
    "    \"QQP_triplets\": (load_QQP_triplets(), \"embedding-data/QQP_triplets\"),\n",
    "    \"Sentence-compression\": (load_sentence_compression(), \"embedding-data/sentence-compression\"),\n",
    "    \"Altlex\": (load_altlex(), \"embedding-data/altlex\"),\n",
    "    \"AG-news\": (load_agnews(), \"fancyzhx/ag_news\"),\n",
    "    \"SST2\": (load_sst2(), \"stanfordnlp/sst2\"),\n",
    "    \"DAIR-emotion\": (load_dair_emotion(), \"dair-ai/emotion\"),\n",
    "    \"SNLI\": (load_snli(), \"stanfordnlp/snli\"),\n",
    "    \"Tweet_eval\": (tweet_eval(), \"cardiffnlp/tweet_eval\"),\n",
    "    \"IMDB\": (load_imdb(), \"stanfordnlp/imdb\"),\n",
    "}\n",
    "\n",
    "# Make huggingface url\n",
    "hfbase = \"https://huggingface.co/datasets/\"\n",
    "\n",
    "# Make dataframe with Name, URL, Number of samples\n",
    "\n",
    "data = []\n",
    "for name, (df, url) in datasets.items():\n",
    "    data.append((name, \"\\\\url{\" + hfbase + url + \"}\", len(df)))\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"Name\", \"URL\", \"Number of samples\"])\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ],
   "id": "8e2d6dcab1dc351e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# add line for total number of samples\n",
    "total_samples = df[\"Number of samples\"].sum()\n",
    "\n",
    "data.append((\"Total\", \"Total\", total_samples))\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Name\", \"URL\", \"Number of samples\"])"
   ],
   "id": "6c063dfc0d6fdbbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "idx = pd.IndexSlice\n",
    "\n",
    "\n",
    "style = df[['URL', \"Number of samples\"]].set_index(\"URL\").style\n",
    "style = style.format(escape=\"latex\", subset=[\"Number of samples\"])\n",
    "\n",
    "# hide index\n",
    "# style = style.hide(axis=0)\n",
    "\n",
    "         \n",
    "latex= style.to_latex(caption=\"Number of samples in each dataset\", clines=\"skip-last;index\", hrules=True)\n",
    "\n",
    "# add resize box\n",
    "latex = latex.replace(\"\\\\begin{tabular}\", \"\\\\centering \\\\resizebox{\\\\textwidth}{!}{ \\\\begin{tabular}\")\n",
    "latex = latex.replace(\"\\\\end{tabular}\", \"\\\\end{tabular}\\n}\")\n",
    "\n",
    "\n",
    "\n",
    "print(latex)\n",
    "\n",
    "with open(paper_path / \"training_datasets.tex\", \"w\") as f:\n",
    "    f.write(latex)\n",
    "    "
   ],
   "id": "8789f9d5ba22b914",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c5ae190baf377991",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
