{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 13.585102081298828,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.41024000000000005,
        "f1": 0.40651487119449004,
        "f1_weighted": 0.40651487119449004,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.41024000000000005,
        "scores_per_experiment": [
          {
            "accuracy": 0.4104,
            "f1": 0.4083628834455637,
            "f1_weighted": 0.4083628834455637
          },
          {
            "accuracy": 0.437,
            "f1": 0.433305686328751,
            "f1_weighted": 0.433305686328751
          },
          {
            "accuracy": 0.3994,
            "f1": 0.3983529769556786,
            "f1_weighted": 0.3983529769556786
          },
          {
            "accuracy": 0.399,
            "f1": 0.40164940201507404,
            "f1_weighted": 0.40164940201507404
          },
          {
            "accuracy": 0.4318,
            "f1": 0.41402598369103244,
            "f1_weighted": 0.41402598369103244
          },
          {
            "accuracy": 0.3968,
            "f1": 0.3967640887511239,
            "f1_weighted": 0.3967640887511239
          },
          {
            "accuracy": 0.3762,
            "f1": 0.3775600141130103,
            "f1_weighted": 0.3775600141130104
          },
          {
            "accuracy": 0.4354,
            "f1": 0.4329721711212354,
            "f1_weighted": 0.43297217112123554
          },
          {
            "accuracy": 0.4202,
            "f1": 0.41216997051601884,
            "f1_weighted": 0.41216997051601884
          },
          {
            "accuracy": 0.3962,
            "f1": 0.3899855350074117,
            "f1_weighted": 0.38998553500741173
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}