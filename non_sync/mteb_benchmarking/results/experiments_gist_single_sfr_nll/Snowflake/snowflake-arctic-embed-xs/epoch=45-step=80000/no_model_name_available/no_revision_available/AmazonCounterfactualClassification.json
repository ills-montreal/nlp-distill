{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 7.59011697769165,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7613432835820896,
        "ap": 0.3930891450004906,
        "ap_weighted": 0.3930891450004906,
        "f1": 0.7014210475069456,
        "f1_weighted": 0.7826266949048514,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7613432835820896,
        "scores_per_experiment": [
          {
            "accuracy": 0.7149253731343284,
            "ap": 0.3230068528587197,
            "ap_weighted": 0.3230068528587197,
            "f1": 0.6463696027633851,
            "f1_weighted": 0.7411857190730287
          },
          {
            "accuracy": 0.7970149253731343,
            "ap": 0.43721599594167604,
            "ap_weighted": 0.43721599594167604,
            "f1": 0.7379922708870077,
            "f1_weighted": 0.8137194502276984
          },
          {
            "accuracy": 0.7179104477611941,
            "ap": 0.3679175181425987,
            "ap_weighted": 0.3679175181425987,
            "f1": 0.6692446931051506,
            "f1_weighted": 0.7465039456330328
          },
          {
            "accuracy": 0.746268656716418,
            "ap": 0.36344386868440676,
            "ap_weighted": 0.36344386868440676,
            "f1": 0.6815731794596529,
            "f1_weighted": 0.7689763407866732
          },
          {
            "accuracy": 0.764179104477612,
            "ap": 0.4121355627198467,
            "ap_weighted": 0.4121355627198467,
            "f1": 0.7116348500697349,
            "f1_weighted": 0.7865930871271258
          },
          {
            "accuracy": 0.7716417910447761,
            "ap": 0.38783752990771336,
            "ap_weighted": 0.38783752990771336,
            "f1": 0.7047004225972882,
            "f1_weighted": 0.7903182229940753
          },
          {
            "accuracy": 0.8,
            "ap": 0.4246308950499782,
            "ap_weighted": 0.4246308950499782,
            "f1": 0.7338621703661545,
            "f1_weighted": 0.8146532915955227
          },
          {
            "accuracy": 0.808955223880597,
            "ap": 0.44770095612008987,
            "ap_weighted": 0.44770095612008987,
            "f1": 0.7477647058823529,
            "f1_weighted": 0.8234184372256363
          },
          {
            "accuracy": 0.744776119402985,
            "ap": 0.39330658308069366,
            "ap_weighted": 0.39330658308069366,
            "f1": 0.6939666801114404,
            "f1_weighted": 0.7699016663053974
          },
          {
            "accuracy": 0.7477611940298508,
            "ap": 0.37369568749918286,
            "ap_weighted": 0.37369568749918286,
            "f1": 0.6871018998272884,
            "f1_weighted": 0.7709967880803238
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}