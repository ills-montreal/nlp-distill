{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 9.072351455688477,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7792537313432837,
        "ap": 0.411496931634278,
        "ap_weighted": 0.411496931634278,
        "f1": 0.7178226966794494,
        "f1_weighted": 0.7978594077447017,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7792537313432837,
        "scores_per_experiment": [
          {
            "accuracy": 0.7253731343283583,
            "ap": 0.3159781144256467,
            "ap_weighted": 0.3159781144256467,
            "f1": 0.646806706317828,
            "f1_weighted": 0.7482469045086391
          },
          {
            "accuracy": 0.7895522388059701,
            "ap": 0.4236692543061462,
            "ap_weighted": 0.4236692543061462,
            "f1": 0.7288547146904087,
            "f1_weighted": 0.8069764491987654
          },
          {
            "accuracy": 0.7507462686567165,
            "ap": 0.3764387010754065,
            "ap_weighted": 0.3764387010754065,
            "f1": 0.689789875544441,
            "f1_weighted": 0.7735279509310011
          },
          {
            "accuracy": 0.7567164179104477,
            "ap": 0.39109303603005263,
            "ap_weighted": 0.39109303603005263,
            "f1": 0.69918826831492,
            "f1_weighted": 0.7792957257721737
          },
          {
            "accuracy": 0.7865671641791044,
            "ap": 0.4317885031682457,
            "ap_weighted": 0.4317885031682457,
            "f1": 0.7307762557077626,
            "f1_weighted": 0.8054080283513937
          },
          {
            "accuracy": 0.8134328358208955,
            "ap": 0.4559985232381837,
            "ap_weighted": 0.4559985232381837,
            "f1": 0.7531966558515231,
            "f1_weighted": 0.8274454819164896
          },
          {
            "accuracy": 0.8223880597014925,
            "ap": 0.45231056024086164,
            "ap_weighted": 0.45231056024086164,
            "f1": 0.7552034240203378,
            "f1_weighted": 0.8332983851539878
          },
          {
            "accuracy": 0.808955223880597,
            "ap": 0.4453774935478598,
            "ap_weighted": 0.4453774935478598,
            "f1": 0.7467786320849427,
            "f1_weighted": 0.8231884195928554
          },
          {
            "accuracy": 0.7820895522388059,
            "ap": 0.426700683460656,
            "ap_weighted": 0.426700683460656,
            "f1": 0.7265275541241725,
            "f1_weighted": 0.80159144561679
          },
          {
            "accuracy": 0.7567164179104477,
            "ap": 0.3956144468497209,
            "ap_weighted": 0.3956144468497209,
            "f1": 0.7011048801381579,
            "f1_weighted": 0.7796152864049201
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}