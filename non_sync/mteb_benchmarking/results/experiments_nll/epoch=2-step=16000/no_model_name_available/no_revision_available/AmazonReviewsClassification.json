{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 33.510034799575806,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.46264000000000005,
        "f1": 0.4579784620139334,
        "f1_weighted": 0.4579784620139334,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.46264000000000005,
        "scores_per_experiment": [
          {
            "accuracy": 0.4754,
            "f1": 0.45440019813020677,
            "f1_weighted": 0.4544001981302067
          },
          {
            "accuracy": 0.4878,
            "f1": 0.48072720912944583,
            "f1_weighted": 0.4807272091294459
          },
          {
            "accuracy": 0.4504,
            "f1": 0.44904052015709245,
            "f1_weighted": 0.4490405201570924
          },
          {
            "accuracy": 0.4514,
            "f1": 0.4527477994946761,
            "f1_weighted": 0.4527477994946761
          },
          {
            "accuracy": 0.4874,
            "f1": 0.4745976468485097,
            "f1_weighted": 0.4745976468485097
          },
          {
            "accuracy": 0.4484,
            "f1": 0.45347333849590754,
            "f1_weighted": 0.4534733384959076
          },
          {
            "accuracy": 0.431,
            "f1": 0.43067092527207906,
            "f1_weighted": 0.43067092527207906
          },
          {
            "accuracy": 0.475,
            "f1": 0.4802063496603556,
            "f1_weighted": 0.4802063496603556
          },
          {
            "accuracy": 0.4666,
            "f1": 0.46181419500260334,
            "f1_weighted": 0.46181419500260334
          },
          {
            "accuracy": 0.453,
            "f1": 0.4421064379484571,
            "f1_weighted": 0.44210643794845716
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}