{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 19.119492292404175,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7728358208955225,
        "ap": 0.40325507086890217,
        "ap_weighted": 0.40325507086890217,
        "f1": 0.7115388854921605,
        "f1_weighted": 0.7923501104285331,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7728358208955225,
        "scores_per_experiment": [
          {
            "accuracy": 0.7432835820895523,
            "ap": 0.33440943692685166,
            "ap_weighted": 0.33440943692685166,
            "f1": 0.6649259720170735,
            "f1_weighted": 0.7635985180342689
          },
          {
            "accuracy": 0.7910447761194029,
            "ap": 0.42541747069288977,
            "ap_weighted": 0.42541747069288977,
            "f1": 0.7302861612072138,
            "f1_weighted": 0.8082406105285131
          },
          {
            "accuracy": 0.7343283582089553,
            "ap": 0.3728765252771351,
            "ap_weighted": 0.3728765252771351,
            "f1": 0.6800313368605189,
            "f1_weighted": 0.7602964988538596
          },
          {
            "accuracy": 0.7895522388059701,
            "ap": 0.4213528126197604,
            "ap_weighted": 0.4213528126197604,
            "f1": 0.7278611737661282,
            "f1_weighted": 0.8067638525631674
          },
          {
            "accuracy": 0.7671641791044777,
            "ap": 0.4014984962122855,
            "ap_weighted": 0.4014984962122855,
            "f1": 0.7087768180551686,
            "f1_weighted": 0.7881836290822288
          },
          {
            "accuracy": 0.7746268656716417,
            "ap": 0.40701598613932183,
            "ap_weighted": 0.40701598613932183,
            "f1": 0.7147336395498674,
            "f1_weighted": 0.7943311517898802
          },
          {
            "accuracy": 0.8238805970149253,
            "ap": 0.46370817667388253,
            "ap_weighted": 0.46370817667388253,
            "f1": 0.760833000568725,
            "f1_weighted": 0.8356103824002651
          },
          {
            "accuracy": 0.7850746268656716,
            "ap": 0.4046530598271945,
            "ap_weighted": 0.4046530598271945,
            "f1": 0.7184085177920987,
            "f1_weighted": 0.8018434027676133
          },
          {
            "accuracy": 0.7597014925373134,
            "ap": 0.4030786633897036,
            "ap_weighted": 0.4030786633897036,
            "f1": 0.7057001999830846,
            "f1_weighted": 0.7824685880044273
          },
          {
            "accuracy": 0.7597014925373134,
            "ap": 0.3985400809299967,
            "ap_weighted": 0.3985400809299967,
            "f1": 0.7038320351217262,
            "f1_weighted": 0.7821644702611061
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}