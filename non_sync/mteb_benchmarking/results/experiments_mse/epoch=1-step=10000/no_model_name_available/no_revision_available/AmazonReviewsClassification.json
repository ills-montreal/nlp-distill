{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 34.57450270652771,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.4556400000000001,
        "f1": 0.4507742370339832,
        "f1_weighted": 0.4507742370339832,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4556400000000001,
        "scores_per_experiment": [
          {
            "accuracy": 0.4686,
            "f1": 0.44779197310979224,
            "f1_weighted": 0.4477919731097922
          },
          {
            "accuracy": 0.4726,
            "f1": 0.4683039051406447,
            "f1_weighted": 0.46830390514064474
          },
          {
            "accuracy": 0.4242,
            "f1": 0.4231468905802571,
            "f1_weighted": 0.4231468905802572
          },
          {
            "accuracy": 0.4662,
            "f1": 0.4666139047554566,
            "f1_weighted": 0.46661390475545667
          },
          {
            "accuracy": 0.462,
            "f1": 0.4472827539746933,
            "f1_weighted": 0.44728275397469336
          },
          {
            "accuracy": 0.4352,
            "f1": 0.43794092387734374,
            "f1_weighted": 0.43794092387734374
          },
          {
            "accuracy": 0.4452,
            "f1": 0.4388912741470257,
            "f1_weighted": 0.4388912741470257
          },
          {
            "accuracy": 0.4672,
            "f1": 0.47053743049507324,
            "f1_weighted": 0.4705374304950732
          },
          {
            "accuracy": 0.4626,
            "f1": 0.46102636054573204,
            "f1_weighted": 0.46102636054573215
          },
          {
            "accuracy": 0.4526,
            "f1": 0.44620695371381336,
            "f1_weighted": 0.44620695371381336
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}