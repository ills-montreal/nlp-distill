{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 16.67854619026184,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7583582089552239,
        "ap": 0.3912784732497768,
        "ap_weighted": 0.3912784732497768,
        "f1": 0.6994416693762187,
        "f1_weighted": 0.7803275714220699,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7583582089552239,
        "scores_per_experiment": [
          {
            "accuracy": 0.735820895522388,
            "ap": 0.33262249955240153,
            "ap_weighted": 0.33262249955240153,
            "f1": 0.660856509616072,
            "f1_weighted": 0.7579532380280622
          },
          {
            "accuracy": 0.7671641791044777,
            "ap": 0.4083490477198057,
            "ap_weighted": 0.4083490477198057,
            "f1": 0.7116403284188223,
            "f1_weighted": 0.788693835492793
          },
          {
            "accuracy": 0.7597014925373134,
            "ap": 0.38042365822899193,
            "ap_weighted": 0.38042365822899193,
            "f1": 0.6958418275995274,
            "f1_weighted": 0.7807106982660312
          },
          {
            "accuracy": 0.744776119402985,
            "ap": 0.36433481447723215,
            "ap_weighted": 0.36433481447723215,
            "f1": 0.6813064848580943,
            "f1_weighted": 0.7679138791534639
          },
          {
            "accuracy": 0.7731343283582089,
            "ap": 0.416901919579372,
            "ap_weighted": 0.416901919579372,
            "f1": 0.7181164956101017,
            "f1_weighted": 0.7939518866953307
          },
          {
            "accuracy": 0.753731343283582,
            "ap": 0.3837252543439728,
            "ap_weighted": 0.3837252543439728,
            "f1": 0.6945077720207254,
            "f1_weighted": 0.7764169824452866
          },
          {
            "accuracy": 0.808955223880597,
            "ap": 0.44305404646845825,
            "ap_weighted": 0.44305404646845825,
            "f1": 0.7457787896034908,
            "f1_weighted": 0.8229523979419918
          },
          {
            "accuracy": 0.753731343283582,
            "ap": 0.3995044306251452,
            "ap_weighted": 0.3995044306251452,
            "f1": 0.70116694914567,
            "f1_weighted": 0.7774882046911082
          },
          {
            "accuracy": 0.7417910447761195,
            "ap": 0.39503573846081674,
            "ap_weighted": 0.39503573846081674,
            "f1": 0.6930438996745327,
            "f1_weighted": 0.7675339191556091
          },
          {
            "accuracy": 0.744776119402985,
            "ap": 0.38883332304157203,
            "ap_weighted": 0.38883332304157203,
            "f1": 0.6921576372151511,
            "f1_weighted": 0.7696606723510221
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}