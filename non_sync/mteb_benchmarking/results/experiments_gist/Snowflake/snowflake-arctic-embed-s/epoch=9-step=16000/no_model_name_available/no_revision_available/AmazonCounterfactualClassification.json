{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 7.911394834518433,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.6586567164179103,
        "ap": 0.2890673797957134,
        "ap_weighted": 0.2890673797957134,
        "f1": 0.5994498940166932,
        "f1_weighted": 0.6928659048613725,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6586567164179103,
        "scores_per_experiment": [
          {
            "accuracy": 0.5895522388059702,
            "ap": 0.24278577751290523,
            "ap_weighted": 0.24278577751290523,
            "f1": 0.5363275125764084,
            "f1_weighted": 0.6319913376938586
          },
          {
            "accuracy": 0.6582089552238806,
            "ap": 0.2836115157093876,
            "ap_weighted": 0.2836115157093876,
            "f1": 0.5970628632356301,
            "f1_weighted": 0.692647558757493
          },
          {
            "accuracy": 0.6776119402985075,
            "ap": 0.296457074777071,
            "ap_weighted": 0.296457074777071,
            "f1": 0.6139935345517396,
            "f1_weighted": 0.7094211431718913
          },
          {
            "accuracy": 0.6701492537313433,
            "ap": 0.2715392503133189,
            "ap_weighted": 0.2715392503133189,
            "f1": 0.5947495614143123,
            "f1_weighted": 0.7011961858618855
          },
          {
            "accuracy": 0.7298507462686568,
            "ap": 0.3537849646713723,
            "ap_weighted": 0.3537849646713723,
            "f1": 0.6691412186145238,
            "f1_weighted": 0.7554460523528034
          },
          {
            "accuracy": 0.6865671641791045,
            "ap": 0.29796146543251306,
            "ap_weighted": 0.29796146543251306,
            "f1": 0.6190873256519102,
            "f1_weighted": 0.7167177303295529
          },
          {
            "accuracy": 0.6417910447761194,
            "ap": 0.2787232454044293,
            "ap_weighted": 0.2787232454044293,
            "f1": 0.5859767652632446,
            "f1_weighted": 0.6785467898211833
          },
          {
            "accuracy": 0.6582089552238806,
            "ap": 0.29294359943286113,
            "ap_weighted": 0.29294359943286113,
            "f1": 0.6024089204688273,
            "f1_weighted": 0.6931117658077187
          },
          {
            "accuracy": 0.6402985074626866,
            "ap": 0.2925134240151128,
            "ap_weighted": 0.2925134240151128,
            "f1": 0.592719549814735,
            "f1_weighted": 0.6774890464102121
          },
          {
            "accuracy": 0.6343283582089553,
            "ap": 0.28035348068816224,
            "ap_weighted": 0.28035348068816224,
            "f1": 0.5830316885756017,
            "f1_weighted": 0.6720914384071262
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}