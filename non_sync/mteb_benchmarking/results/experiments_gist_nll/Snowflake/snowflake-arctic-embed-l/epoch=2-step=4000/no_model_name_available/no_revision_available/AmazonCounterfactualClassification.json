{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 11.588757514953613,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.6477611940298507,
        "ap": 0.28213524005396956,
        "ap_weighted": 0.28213524005396956,
        "f1": 0.5904206887458843,
        "f1_weighted": 0.6836032843904424,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6477611940298507,
        "scores_per_experiment": [
          {
            "accuracy": 0.6298507462686567,
            "ap": 0.2608263044169529,
            "ap_weighted": 0.2608263044169529,
            "f1": 0.5688590436849371,
            "f1_weighted": 0.667607514534769
          },
          {
            "accuracy": 0.6328358208955224,
            "ap": 0.27783240099103906,
            "ap_weighted": 0.27783240099103906,
            "f1": 0.5808282723471786,
            "f1_weighted": 0.6707396274646543
          },
          {
            "accuracy": 0.6402985074626866,
            "ap": 0.2815674432795294,
            "ap_weighted": 0.2815674432795294,
            "f1": 0.5868419907936921,
            "f1_weighted": 0.6773409899677573
          },
          {
            "accuracy": 0.6567164179104478,
            "ap": 0.27914093653868066,
            "ap_weighted": 0.27914093653868066,
            "f1": 0.5936224301431419,
            "f1_weighted": 0.6911313203289782
          },
          {
            "accuracy": 0.6597014925373135,
            "ap": 0.30521166792349874,
            "ap_weighted": 0.30521166792349874,
            "f1": 0.6096089956555073,
            "f1_weighted": 0.6947662403545777
          },
          {
            "accuracy": 0.6656716417910448,
            "ap": 0.27285512320182437,
            "ap_weighted": 0.27285512320182437,
            "f1": 0.5936931473620376,
            "f1_weighted": 0.6978322456848565
          },
          {
            "accuracy": 0.6283582089552239,
            "ap": 0.2568100525220366,
            "ap_weighted": 0.2568100525220366,
            "f1": 0.5654054054054054,
            "f1_weighted": 0.666129891085115
          },
          {
            "accuracy": 0.6447761194029851,
            "ap": 0.27663675134448246,
            "ap_weighted": 0.27663675134448246,
            "f1": 0.5862437596653831,
            "f1_weighted": 0.6810104373357863
          },
          {
            "accuracy": 0.6597014925373135,
            "ap": 0.31100305007312323,
            "ap_weighted": 0.31100305007312323,
            "f1": 0.6124259154014776,
            "f1_weighted": 0.6948551268178067
          },
          {
            "accuracy": 0.6597014925373135,
            "ap": 0.2994686702485281,
            "ap_weighted": 0.2994686702485281,
            "f1": 0.6066779270000824,
            "f1_weighted": 0.6946194503301241
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}