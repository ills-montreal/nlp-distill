{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 7.799670696258545,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7962686567164179,
        "ap": 0.4385446669440505,
        "ap_weighted": 0.4385446669440505,
        "f1": 0.7374917628833165,
        "f1_weighted": 0.8129677736378834,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7962686567164179,
        "scores_per_experiment": [
          {
            "accuracy": 0.7746268656716417,
            "ap": 0.3795045617479251,
            "ap_weighted": 0.3795045617479251,
            "f1": 0.7018615602686399,
            "f1_weighted": 0.7915541421551195
          },
          {
            "accuracy": 0.8238805970149253,
            "ap": 0.48910731808995966,
            "ap_weighted": 0.48910731808995966,
            "f1": 0.7709839636633297,
            "f1_weighted": 0.8380082692765318
          },
          {
            "accuracy": 0.7597014925373134,
            "ap": 0.40989292839901326,
            "ap_weighted": 0.40989292839901326,
            "f1": 0.7084113867421387,
            "f1_weighted": 0.7828824300319299
          },
          {
            "accuracy": 0.7865671641791044,
            "ap": 0.41328064488817207,
            "ap_weighted": 0.41328064488817207,
            "f1": 0.7229795957312084,
            "f1_weighted": 0.8038011780575062
          },
          {
            "accuracy": 0.8044776119402985,
            "ap": 0.45583912498575824,
            "ap_weighted": 0.45583912498575824,
            "f1": 0.7489954958175449,
            "f1_weighted": 0.8208580462241591
          },
          {
            "accuracy": 0.8298507462686567,
            "ap": 0.4840530173559683,
            "ap_weighted": 0.4840530173559683,
            "f1": 0.7717656339644292,
            "f1_weighted": 0.8418802073967513
          },
          {
            "accuracy": 0.8328358208955224,
            "ap": 0.4680138382777186,
            "ap_weighted": 0.4680138382777186,
            "f1": 0.7660381123416142,
            "f1_weighted": 0.8421651097996994
          },
          {
            "accuracy": 0.7940298507462686,
            "ap": 0.43592318306685407,
            "ap_weighted": 0.43592318306685407,
            "f1": 0.7360578240613866,
            "f1_weighted": 0.811384533893972
          },
          {
            "accuracy": 0.7746268656716417,
            "ap": 0.41620333447267477,
            "ap_weighted": 0.41620333447267477,
            "f1": 0.7185805801963288,
            "f1_weighted": 0.7950584546910703
          },
          {
            "accuracy": 0.7820895522388059,
            "ap": 0.4336287181564611,
            "ap_weighted": 0.4336287181564611,
            "f1": 0.7292434760465452,
            "f1_weighted": 0.8020853648520939
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}