{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 13.644542455673218,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.44108,
        "f1": 0.43762996264739307,
        "f1_weighted": 0.43762996264739307,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.44108,
        "scores_per_experiment": [
          {
            "accuracy": 0.4658,
            "f1": 0.4556853242530078,
            "f1_weighted": 0.45568532425300784
          },
          {
            "accuracy": 0.451,
            "f1": 0.45331049591423433,
            "f1_weighted": 0.45331049591423433
          },
          {
            "accuracy": 0.4318,
            "f1": 0.43113764243930514,
            "f1_weighted": 0.4311376424393051
          },
          {
            "accuracy": 0.4342,
            "f1": 0.4366565819210392,
            "f1_weighted": 0.4366565819210391
          },
          {
            "accuracy": 0.4556,
            "f1": 0.436620165908728,
            "f1_weighted": 0.436620165908728
          },
          {
            "accuracy": 0.4184,
            "f1": 0.4188616597638045,
            "f1_weighted": 0.4188616597638045
          },
          {
            "accuracy": 0.4128,
            "f1": 0.4176220112428597,
            "f1_weighted": 0.41762201124285964
          },
          {
            "accuracy": 0.4566,
            "f1": 0.4579475951015414,
            "f1_weighted": 0.4579475951015414
          },
          {
            "accuracy": 0.455,
            "f1": 0.4484087335553656,
            "f1_weighted": 0.4484087335553656
          },
          {
            "accuracy": 0.4296,
            "f1": 0.42004941637404497,
            "f1_weighted": 0.42004941637404497
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}