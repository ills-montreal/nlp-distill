{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 9.07312297821045,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7729850746268657,
        "ap": 0.4089558265919055,
        "ap_weighted": 0.4089558265919055,
        "f1": 0.7138372439331528,
        "f1_weighted": 0.7928802223885549,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7729850746268657,
        "scores_per_experiment": [
          {
            "accuracy": 0.7328358208955223,
            "ap": 0.3323872801520721,
            "ap_weighted": 0.3323872801520721,
            "f1": 0.659462943559951,
            "f1_weighted": 0.7557206090034014
          },
          {
            "accuracy": 0.8134328358208955,
            "ap": 0.4583196341086281,
            "ap_weighted": 0.4583196341086281,
            "f1": 0.7541529799594315,
            "f1_weighted": 0.8276672085049552
          },
          {
            "accuracy": 0.7179104477611941,
            "ap": 0.3592851755043249,
            "ap_weighted": 0.3592851755043249,
            "f1": 0.6655971479500892,
            "f1_weighted": 0.7461398887913374
          },
          {
            "accuracy": 0.7701492537313432,
            "ap": 0.3931507509364433,
            "ap_weighted": 0.3931507509364433,
            "f1": 0.7065047958220028,
            "f1_weighted": 0.7897321638572942
          },
          {
            "accuracy": 0.7656716417910447,
            "ap": 0.3999748395427443,
            "ap_weighted": 0.3999748395427443,
            "f1": 0.7073983515948585,
            "f1_weighted": 0.7869150820297883
          },
          {
            "accuracy": 0.8194029850746268,
            "ap": 0.4688678232443187,
            "ap_weighted": 0.4688678232443187,
            "f1": 0.7610943628642743,
            "f1_weighted": 0.832967226495162
          },
          {
            "accuracy": 0.8059701492537313,
            "ap": 0.4461351986439162,
            "ap_weighted": 0.4461351986439162,
            "f1": 0.7457854674512001,
            "f1_weighted": 0.8211086274985399
          },
          {
            "accuracy": 0.7910447761194029,
            "ap": 0.4393269865983127,
            "ap_weighted": 0.4393269865983127,
            "f1": 0.7359709962168979,
            "f1_weighted": 0.8094027027535714
          },
          {
            "accuracy": 0.7701492537313432,
            "ap": 0.4068729939367013,
            "ap_weighted": 0.4068729939367013,
            "f1": 0.7125104485929228,
            "f1_weighted": 0.7908992235811745
          },
          {
            "accuracy": 0.7432835820895523,
            "ap": 0.3852375832515932,
            "ap_weighted": 0.3852375832515932,
            "f1": 0.6898949453199001,
            "f1_weighted": 0.7682494913703248
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}