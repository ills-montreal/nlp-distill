{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 15.703729629516602,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.42421999999999993,
        "f1": 0.4210091649332086,
        "f1_weighted": 0.4210091649332087,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.42421999999999993,
        "scores_per_experiment": [
          {
            "accuracy": 0.4304,
            "f1": 0.4283352837366114,
            "f1_weighted": 0.4283352837366114
          },
          {
            "accuracy": 0.453,
            "f1": 0.4476176061290963,
            "f1_weighted": 0.4476176061290964
          },
          {
            "accuracy": 0.4084,
            "f1": 0.41020938927309103,
            "f1_weighted": 0.4102093892730911
          },
          {
            "accuracy": 0.4082,
            "f1": 0.4122440030487189,
            "f1_weighted": 0.4122440030487189
          },
          {
            "accuracy": 0.4516,
            "f1": 0.4344732734358402,
            "f1_weighted": 0.43447327343584013
          },
          {
            "accuracy": 0.4078,
            "f1": 0.4083443243590724,
            "f1_weighted": 0.4083443243590725
          },
          {
            "accuracy": 0.3816,
            "f1": 0.3852784856896684,
            "f1_weighted": 0.3852784856896683
          },
          {
            "accuracy": 0.4474,
            "f1": 0.4487982436129908,
            "f1_weighted": 0.4487982436129908
          },
          {
            "accuracy": 0.4398,
            "f1": 0.4310710431938006,
            "f1_weighted": 0.4310710431938006
          },
          {
            "accuracy": 0.414,
            "f1": 0.40371999685319615,
            "f1_weighted": 0.40371999685319615
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}