{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 12.914118766784668,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.42518,
        "f1": 0.4216597121047861,
        "f1_weighted": 0.4216597121047861,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.42518,
        "scores_per_experiment": [
          {
            "accuracy": 0.427,
            "f1": 0.42249491502686753,
            "f1_weighted": 0.4224949150268676
          },
          {
            "accuracy": 0.4466,
            "f1": 0.44487093464549493,
            "f1_weighted": 0.444870934645495
          },
          {
            "accuracy": 0.4102,
            "f1": 0.410001910098486,
            "f1_weighted": 0.410001910098486
          },
          {
            "accuracy": 0.412,
            "f1": 0.4160896500126755,
            "f1_weighted": 0.4160896500126755
          },
          {
            "accuracy": 0.4476,
            "f1": 0.4309871851379893,
            "f1_weighted": 0.4309871851379893
          },
          {
            "accuracy": 0.4092,
            "f1": 0.4083808956800822,
            "f1_weighted": 0.40838089568008223
          },
          {
            "accuracy": 0.3934,
            "f1": 0.39723383421292785,
            "f1_weighted": 0.39723383421292785
          },
          {
            "accuracy": 0.4514,
            "f1": 0.44919906373743634,
            "f1_weighted": 0.44919906373743634
          },
          {
            "accuracy": 0.44,
            "f1": 0.43117604426839834,
            "f1_weighted": 0.43117604426839834
          },
          {
            "accuracy": 0.4144,
            "f1": 0.4061626882275025,
            "f1_weighted": 0.40616268822750257
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}