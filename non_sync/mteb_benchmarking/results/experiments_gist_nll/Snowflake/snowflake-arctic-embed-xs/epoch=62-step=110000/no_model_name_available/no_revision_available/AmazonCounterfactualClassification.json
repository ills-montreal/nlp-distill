{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 9.500176191329956,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7597014925373134,
        "ap": 0.39541610135923855,
        "ap_weighted": 0.39541610135923855,
        "f1": 0.7017376208581562,
        "f1_weighted": 0.7816193860984227,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7597014925373134,
        "scores_per_experiment": [
          {
            "accuracy": 0.7283582089552239,
            "ap": 0.33959495229184916,
            "ap_weighted": 0.33959495229184916,
            "f1": 0.6613680796232115,
            "f1_weighted": 0.7530861090442216
          },
          {
            "accuracy": 0.7970149253731343,
            "ap": 0.4395383163598705,
            "ap_weighted": 0.4395383163598705,
            "f1": 0.7389440872783948,
            "f1_weighted": 0.8139216250716027
          },
          {
            "accuracy": 0.6970149253731344,
            "ap": 0.34348866355246666,
            "ap_weighted": 0.34348866355246666,
            "f1": 0.6475502657431089,
            "f1_weighted": 0.7279549714365366
          },
          {
            "accuracy": 0.7328358208955223,
            "ap": 0.34536857696251566,
            "ap_weighted": 0.34536857696251566,
            "f1": 0.6663968467228003,
            "f1_weighted": 0.7570560489384209
          },
          {
            "accuracy": 0.7656716417910447,
            "ap": 0.4159483582268976,
            "ap_weighted": 0.4159483582268976,
            "f1": 0.713904943224315,
            "f1_weighted": 0.7880130590672124
          },
          {
            "accuracy": 0.7791044776119403,
            "ap": 0.41185925102976073,
            "ap_weighted": 0.41185925102976073,
            "f1": 0.7189342403628118,
            "f1_weighted": 0.7981260364842454
          },
          {
            "accuracy": 0.7820895522388059,
            "ap": 0.41054463076395653,
            "ap_weighted": 0.41054463076395653,
            "f1": 0.7197487995782768,
            "f1_weighted": 0.8002393916209855
          },
          {
            "accuracy": 0.7910447761194029,
            "ap": 0.4300535490486499,
            "ap_weighted": 0.4300535490486499,
            "f1": 0.7322325751347402,
            "f1_weighted": 0.8086509764141748
          },
          {
            "accuracy": 0.753731343283582,
            "ap": 0.4040223971918277,
            "ap_weighted": 0.4040223971918277,
            "f1": 0.7029591236286545,
            "f1_weighted": 0.7777427540229161
          },
          {
            "accuracy": 0.7701492537313432,
            "ap": 0.41374231816459067,
            "ap_weighted": 0.41374231816459067,
            "f1": 0.7153372472852476,
            "f1_weighted": 0.791402888883911
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}