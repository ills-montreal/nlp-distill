{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 17.927282571792603,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.42778,
        "f1": 0.4244936636414738,
        "f1_weighted": 0.4244936636414738,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.42778,
        "scores_per_experiment": [
          {
            "accuracy": 0.4334,
            "f1": 0.4299822388380325,
            "f1_weighted": 0.4299822388380325
          },
          {
            "accuracy": 0.4544,
            "f1": 0.45195576635362383,
            "f1_weighted": 0.45195576635362383
          },
          {
            "accuracy": 0.4082,
            "f1": 0.4088626279728885,
            "f1_weighted": 0.4088626279728885
          },
          {
            "accuracy": 0.4174,
            "f1": 0.4214648285576983,
            "f1_weighted": 0.4214648285576983
          },
          {
            "accuracy": 0.4522,
            "f1": 0.43606253406479406,
            "f1_weighted": 0.4360625340647941
          },
          {
            "accuracy": 0.4124,
            "f1": 0.4122750562996548,
            "f1_weighted": 0.4122750562996549
          },
          {
            "accuracy": 0.3944,
            "f1": 0.39877563168999347,
            "f1_weighted": 0.39877563168999347
          },
          {
            "accuracy": 0.458,
            "f1": 0.45716259461794584,
            "f1_weighted": 0.4571625946179458
          },
          {
            "accuracy": 0.4384,
            "f1": 0.42916983929435587,
            "f1_weighted": 0.4291698392943558
          },
          {
            "accuracy": 0.409,
            "f1": 0.3992255187257509,
            "f1_weighted": 0.3992255187257509
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}