{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 34.276341676712036,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.46225999999999995,
        "f1": 0.4584880181890849,
        "f1_weighted": 0.4584880181890849,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.46225999999999995,
        "scores_per_experiment": [
          {
            "accuracy": 0.475,
            "f1": 0.4612828483693579,
            "f1_weighted": 0.4612828483693578
          },
          {
            "accuracy": 0.4834,
            "f1": 0.479934625395667,
            "f1_weighted": 0.479934625395667
          },
          {
            "accuracy": 0.4498,
            "f1": 0.44774158436142414,
            "f1_weighted": 0.4477415843614241
          },
          {
            "accuracy": 0.4552,
            "f1": 0.45587124339542767,
            "f1_weighted": 0.4558712433954277
          },
          {
            "accuracy": 0.489,
            "f1": 0.4747078261731293,
            "f1_weighted": 0.47470782617312934
          },
          {
            "accuracy": 0.4384,
            "f1": 0.44348338415587946,
            "f1_weighted": 0.4434833841558795
          },
          {
            "accuracy": 0.4388,
            "f1": 0.4394290783783813,
            "f1_weighted": 0.4394290783783813
          },
          {
            "accuracy": 0.4744,
            "f1": 0.4789654129005744,
            "f1_weighted": 0.4789654129005745
          },
          {
            "accuracy": 0.4634,
            "f1": 0.46126267737643,
            "f1_weighted": 0.46126267737643
          },
          {
            "accuracy": 0.4552,
            "f1": 0.4422015013845778,
            "f1_weighted": 0.44220150138457776
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}