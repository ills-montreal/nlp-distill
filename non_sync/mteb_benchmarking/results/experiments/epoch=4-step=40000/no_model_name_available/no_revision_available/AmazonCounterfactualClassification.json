{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 12.759333610534668,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7823880597014925,
        "ap": 0.41654252730874325,
        "ap_weighted": 0.41654252730874325,
        "f1": 0.7218149454054079,
        "f1_weighted": 0.8007299413136811,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7823880597014925,
        "scores_per_experiment": [
          {
            "accuracy": 0.7507462686567165,
            "ap": 0.3474535718354791,
            "ap_weighted": 0.3474535718354791,
            "f1": 0.6752773738939144,
            "f1_weighted": 0.7706065041206117
          },
          {
            "accuracy": 0.7776119402985074,
            "ap": 0.4171352127961564,
            "ap_weighted": 0.4171352127961564,
            "f1": 0.7204354072211285,
            "f1_weighted": 0.7974255903748269
          },
          {
            "accuracy": 0.7597014925373134,
            "ap": 0.3826846390478611,
            "ap_weighted": 0.3826846390478611,
            "f1": 0.696887952230418,
            "f1_weighted": 0.7809139340180027
          },
          {
            "accuracy": 0.7746268656716417,
            "ap": 0.4116087308315455,
            "ap_weighted": 0.4116087308315455,
            "f1": 0.7166828623516135,
            "f1_weighted": 0.7947064707825426
          },
          {
            "accuracy": 0.7925373134328358,
            "ap": 0.4434181607133641,
            "ap_weighted": 0.4434181607133641,
            "f1": 0.7383069898138392,
            "f1_weighted": 0.8108511604254807
          },
          {
            "accuracy": 0.7925373134328358,
            "ap": 0.4271815799558764,
            "ap_weighted": 0.4271815799558764,
            "f1": 0.731721298960935,
            "f1_weighted": 0.8095047908246827
          },
          {
            "accuracy": 0.826865671641791,
            "ap": 0.4542978037147567,
            "ap_weighted": 0.4542978037147567,
            "f1": 0.7576823306395291,
            "f1_weighted": 0.836528149435403
          },
          {
            "accuracy": 0.7985074626865671,
            "ap": 0.4390473033859157,
            "ap_weighted": 0.4390473033859157,
            "f1": 0.7394415493505484,
            "f1_weighted": 0.8149866673477136
          },
          {
            "accuracy": 0.7925373134328358,
            "ap": 0.43645912662481084,
            "ap_weighted": 0.43645912662481084,
            "f1": 0.7355606098035374,
            "f1_weighted": 0.8103081824104627
          },
          {
            "accuracy": 0.7582089552238805,
            "ap": 0.40613914418166686,
            "ap_weighted": 0.40613914418166686,
            "f1": 0.7061530797886164,
            "f1_weighted": 0.7814679633970838
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}