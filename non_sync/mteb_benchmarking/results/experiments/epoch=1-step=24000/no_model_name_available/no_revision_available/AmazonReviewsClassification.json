{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 37.48834419250488,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.46592,
        "f1": 0.46101551134834534,
        "f1_weighted": 0.4610155113483453,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.46592,
        "scores_per_experiment": [
          {
            "accuracy": 0.4718,
            "f1": 0.45374427112066507,
            "f1_weighted": 0.45374427112066507
          },
          {
            "accuracy": 0.4916,
            "f1": 0.4859278833617549,
            "f1_weighted": 0.4859278833617549
          },
          {
            "accuracy": 0.4498,
            "f1": 0.4482803490859342,
            "f1_weighted": 0.44828034908593417
          },
          {
            "accuracy": 0.454,
            "f1": 0.4546143320006212,
            "f1_weighted": 0.4546143320006211
          },
          {
            "accuracy": 0.4828,
            "f1": 0.46973144978438813,
            "f1_weighted": 0.469731449784388
          },
          {
            "accuracy": 0.446,
            "f1": 0.45048120964078475,
            "f1_weighted": 0.4504812096407847
          },
          {
            "accuracy": 0.45,
            "f1": 0.45015089962222865,
            "f1_weighted": 0.45015089962222865
          },
          {
            "accuracy": 0.4846,
            "f1": 0.48871004781634897,
            "f1_weighted": 0.4887100478163489
          },
          {
            "accuracy": 0.477,
            "f1": 0.47014695174976834,
            "f1_weighted": 0.47014695174976834
          },
          {
            "accuracy": 0.4516,
            "f1": 0.43836771930095886,
            "f1_weighted": 0.4383677193009589
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}