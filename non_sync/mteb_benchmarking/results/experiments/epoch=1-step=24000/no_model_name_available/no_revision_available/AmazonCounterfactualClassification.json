{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 12.915692329406738,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7825373134328358,
        "ap": 0.4178732878797736,
        "ap_weighted": 0.4178732878797736,
        "f1": 0.7220222367401592,
        "f1_weighted": 0.8008976842913075,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7825373134328358,
        "scores_per_experiment": [
          {
            "accuracy": 0.7313432835820896,
            "ap": 0.32476861916413213,
            "ap_weighted": 0.32476861916413213,
            "f1": 0.6544848213978754,
            "f1_weighted": 0.7537197978888862
          },
          {
            "accuracy": 0.7835820895522388,
            "ap": 0.4214508275086566,
            "ap_weighted": 0.4214508275086566,
            "f1": 0.7251138167444309,
            "f1_weighted": 0.8023146429760997
          },
          {
            "accuracy": 0.7417910447761195,
            "ap": 0.3617061685453246,
            "ap_weighted": 0.3617061685453246,
            "f1": 0.6786446016119061,
            "f1_weighted": 0.7653912306051688
          },
          {
            "accuracy": 0.7731343283582089,
            "ap": 0.4054292799019677,
            "ap_weighted": 0.4054292799019677,
            "f1": 0.7133399387497747,
            "f1_weighted": 0.7930657915610202
          },
          {
            "accuracy": 0.7955223880597015,
            "ap": 0.44933085599600353,
            "ap_weighted": 0.44933085599600353,
            "f1": 0.7420723568668774,
            "f1_weighted": 0.813572726462524
          },
          {
            "accuracy": 0.8074626865671641,
            "ap": 0.4503945635310641,
            "ap_weighted": 0.4503945635310641,
            "f1": 0.7482193576578545,
            "f1_weighted": 0.8225925213347725
          },
          {
            "accuracy": 0.8432835820895522,
            "ap": 0.49395172447468216,
            "ap_weighted": 0.49395172447468216,
            "f1": 0.781148604297389,
            "f1_weighted": 0.8521600074884327
          },
          {
            "accuracy": 0.8014925373134328,
            "ap": 0.4404363888783394,
            "ap_weighted": 0.4404363888783394,
            "f1": 0.741388567232878,
            "f1_weighted": 0.8173093715451578
          },
          {
            "accuracy": 0.7820895522388059,
            "ap": 0.42900977879137325,
            "ap_weighted": 0.42900977879137325,
            "f1": 0.7274449707439399,
            "f1_weighted": 0.8017616015769576
          },
          {
            "accuracy": 0.7656716417910447,
            "ap": 0.402254672006192,
            "ap_weighted": 0.402254672006192,
            "f1": 0.7083653320986661,
            "f1_weighted": 0.787089151474055
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}