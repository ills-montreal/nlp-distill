{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 33.7476909160614,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.46175999999999995,
        "f1": 0.4566446753629282,
        "f1_weighted": 0.4566446753629282,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.46175999999999995,
        "scores_per_experiment": [
          {
            "accuracy": 0.4714,
            "f1": 0.4560295065603322,
            "f1_weighted": 0.4560295065603322
          },
          {
            "accuracy": 0.4852,
            "f1": 0.48017527769540946,
            "f1_weighted": 0.4801752776954094
          },
          {
            "accuracy": 0.4502,
            "f1": 0.44926697388998915,
            "f1_weighted": 0.44926697388998926
          },
          {
            "accuracy": 0.4536,
            "f1": 0.45386786429317494,
            "f1_weighted": 0.4538678642931749
          },
          {
            "accuracy": 0.4858,
            "f1": 0.46921065053547545,
            "f1_weighted": 0.46921065053547545
          },
          {
            "accuracy": 0.4344,
            "f1": 0.43905189625934804,
            "f1_weighted": 0.4390518962593481
          },
          {
            "accuracy": 0.4454,
            "f1": 0.4439124877678696,
            "f1_weighted": 0.4439124877678696
          },
          {
            "accuracy": 0.469,
            "f1": 0.47265081297997347,
            "f1_weighted": 0.47265081297997347
          },
          {
            "accuracy": 0.4674,
            "f1": 0.4612389242308801,
            "f1_weighted": 0.4612389242308801
          },
          {
            "accuracy": 0.4552,
            "f1": 0.4410423594168293,
            "f1_weighted": 0.4410423594168293
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}