{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 14.845663070678711,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7898507462686567,
        "ap": 0.4253849966502318,
        "ap_weighted": 0.4253849966502318,
        "f1": 0.7288099250752541,
        "f1_weighted": 0.8069773465915496,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7898507462686567,
        "scores_per_experiment": [
          {
            "accuracy": 0.7731343283582089,
            "ap": 0.3642527279131139,
            "ap_weighted": 0.3642527279131139,
            "f1": 0.6931938686975803,
            "f1_weighted": 0.7885614346085057
          },
          {
            "accuracy": 0.7910447761194029,
            "ap": 0.4323717525631419,
            "ap_weighted": 0.4323717525631419,
            "f1": 0.7331861780200026,
            "f1_weighted": 0.8088474216884494
          },
          {
            "accuracy": 0.7402985074626866,
            "ap": 0.3648214255237396,
            "ap_weighted": 0.3648214255237396,
            "f1": 0.6794046794046794,
            "f1_weighted": 0.764489206280251
          },
          {
            "accuracy": 0.7940298507462686,
            "ap": 0.4312821921960781,
            "ap_weighted": 0.4312821921960781,
            "f1": 0.7341392160471109,
            "f1_weighted": 0.810980030378106
          },
          {
            "accuracy": 0.7910447761194029,
            "ap": 0.4346900618080304,
            "ap_weighted": 0.4346900618080304,
            "f1": 0.7341269841269842,
            "f1_weighted": 0.8090381426202321
          },
          {
            "accuracy": 0.8164179104477612,
            "ap": 0.45544114511713224,
            "ap_weighted": 0.45544114511713224,
            "f1": 0.7542428227631803,
            "f1_weighted": 0.8295171128976284
          },
          {
            "accuracy": 0.8477611940298507,
            "ap": 0.494893050082948,
            "ap_weighted": 0.494893050082948,
            "f1": 0.7830090428774639,
            "f1_weighted": 0.8551917687522558
          },
          {
            "accuracy": 0.7925373134328358,
            "ap": 0.43645912662481084,
            "ap_weighted": 0.43645912662481084,
            "f1": 0.7355606098035374,
            "f1_weighted": 0.8103081824104627
          },
          {
            "accuracy": 0.7925373134328358,
            "ap": 0.44109840374332604,
            "ap_weighted": 0.44109840374332604,
            "f1": 0.7374038138902752,
            "f1_weighted": 0.8106756960184992
          },
          {
            "accuracy": 0.7597014925373134,
            "ap": 0.3985400809299967,
            "ap_weighted": 0.3985400809299967,
            "f1": 0.7038320351217262,
            "f1_weighted": 0.7821644702611061
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}