{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 8.756818771362305,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7662686567164179,
        "ap": 0.3923552375788349,
        "ap_weighted": 0.3923552375788349,
        "f1": 0.7035241748869524,
        "f1_weighted": 0.7865198490028905,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7662686567164179,
        "scores_per_experiment": [
          {
            "accuracy": 0.7164179104477612,
            "ap": 0.30536528553162934,
            "ap_weighted": 0.30536528553162934,
            "f1": 0.6366013519685758,
            "f1_weighted": 0.7403120394192371
          },
          {
            "accuracy": 0.7985074626865671,
            "ap": 0.43672432013195456,
            "ap_weighted": 0.43672432013195456,
            "f1": 0.7384772407252667,
            "f1_weighted": 0.8147773359284151
          },
          {
            "accuracy": 0.7626865671641792,
            "ap": 0.3901497171922764,
            "ap_weighted": 0.3901497171922764,
            "f1": 0.7016726828735533,
            "f1_weighted": 0.7838299924134059
          },
          {
            "accuracy": 0.746268656716418,
            "ap": 0.3612238466591842,
            "ap_weighted": 0.3612238466591842,
            "f1": 0.6804905690016944,
            "f1_weighted": 0.7687716867240865
          },
          {
            "accuracy": 0.7582089552238805,
            "ap": 0.3902804796111798,
            "ap_weighted": 0.3902804796111798,
            "f1": 0.6995715282160295,
            "f1_weighted": 0.7803960897673919
          },
          {
            "accuracy": 0.7865671641791044,
            "ap": 0.41559344153624656,
            "ap_weighted": 0.41559344153624656,
            "f1": 0.7240010485713215,
            "f1_weighted": 0.804022914301652
          },
          {
            "accuracy": 0.8014925373134328,
            "ap": 0.4288168215470487,
            "ap_weighted": 0.4288168215470487,
            "f1": 0.7363695264355302,
            "f1_weighted": 0.8161598821057533
          },
          {
            "accuracy": 0.764179104477612,
            "ap": 0.3984638273268922,
            "ap_weighted": 0.3984638273268922,
            "f1": 0.7060228383542166,
            "f1_weighted": 0.7856461825768518
          },
          {
            "accuracy": 0.7656716417910447,
            "ap": 0.3976957537030991,
            "ap_weighted": 0.3976957537030991,
            "f1": 0.7064183824124544,
            "f1_weighted": 0.7867350927993808
          },
          {
            "accuracy": 0.7626865671641792,
            "ap": 0.39923888254883755,
            "ap_weighted": 0.39923888254883755,
            "f1": 0.7056165803108808,
            "f1_weighted": 0.7845472739927306
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}