{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 7.581458330154419,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.757910447761194,
        "ap": 0.38541621350708466,
        "ap_weighted": 0.38541621350708466,
        "f1": 0.6965042643719239,
        "f1_weighted": 0.7795418659343495,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.757910447761194,
        "scores_per_experiment": [
          {
            "accuracy": 0.7179104477611941,
            "ap": 0.310554399557056,
            "ap_weighted": 0.310554399557056,
            "f1": 0.6404385269990545,
            "f1_weighted": 0.7420737156516363
          },
          {
            "accuracy": 0.7820895522388059,
            "ap": 0.41054463076395653,
            "ap_weighted": 0.41054463076395653,
            "f1": 0.7197487995782768,
            "f1_weighted": 0.8002393916209855
          },
          {
            "accuracy": 0.7507462686567165,
            "ap": 0.3719619555695127,
            "ap_weighted": 0.3719619555695127,
            "f1": 0.6877189163240758,
            "f1_weighted": 0.7731513407483861
          },
          {
            "accuracy": 0.7402985074626866,
            "ap": 0.3560025894990855,
            "ap_weighted": 0.3560025894990855,
            "f1": 0.6751741432153804,
            "f1_weighted": 0.7637432785917168
          },
          {
            "accuracy": 0.7373134328358208,
            "ap": 0.3754707641052437,
            "ap_weighted": 0.3754707641052437,
            "f1": 0.6826831998622234,
            "f1_weighted": 0.7628599446580069
          },
          {
            "accuracy": 0.7955223880597015,
            "ap": 0.43075835194780354,
            "ap_weighted": 0.43075835194780354,
            "f1": 0.7346028294767522,
            "f1_weighted": 0.8120332964606878
          },
          {
            "accuracy": 0.7955223880597015,
            "ap": 0.43075835194780354,
            "ap_weighted": 0.43075835194780354,
            "f1": 0.7346028294767522,
            "f1_weighted": 0.8120332964606878
          },
          {
            "accuracy": 0.7611940298507462,
            "ap": 0.3977494518372463,
            "ap_weighted": 0.3977494518372463,
            "f1": 0.7042464906859716,
            "f1_weighted": 0.783275728710557
          },
          {
            "accuracy": 0.753731343283582,
            "ap": 0.3882279381227328,
            "ap_weighted": 0.3882279381227328,
            "f1": 0.6964738248141914,
            "f1_weighted": 0.7767524074104504
          },
          {
            "accuracy": 0.744776119402985,
            "ap": 0.3821337017204056,
            "ap_weighted": 0.3821337017204056,
            "f1": 0.6893530832865615,
            "f1_weighted": 0.7692562590303806
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}