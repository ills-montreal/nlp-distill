{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 15.030020713806152,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.43041999999999997,
        "f1": 0.42929796294506123,
        "f1_weighted": 0.42929796294506123,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.43041999999999997,
        "scores_per_experiment": [
          {
            "accuracy": 0.442,
            "f1": 0.4404286891122339,
            "f1_weighted": 0.4404286891122339
          },
          {
            "accuracy": 0.4716,
            "f1": 0.474198956813569,
            "f1_weighted": 0.4741989568135691
          },
          {
            "accuracy": 0.4334,
            "f1": 0.4377077790469806,
            "f1_weighted": 0.4377077790469806
          },
          {
            "accuracy": 0.4124,
            "f1": 0.41698833848757033,
            "f1_weighted": 0.4169883384875704
          },
          {
            "accuracy": 0.4362,
            "f1": 0.41792964947640704,
            "f1_weighted": 0.41792964947640704
          },
          {
            "accuracy": 0.4042,
            "f1": 0.4078744776612388,
            "f1_weighted": 0.4078744776612388
          },
          {
            "accuracy": 0.41,
            "f1": 0.414935497620146,
            "f1_weighted": 0.4149354976201459
          },
          {
            "accuracy": 0.4396,
            "f1": 0.44355200339097206,
            "f1_weighted": 0.4435520033909721
          },
          {
            "accuracy": 0.4124,
            "f1": 0.404679876588068,
            "f1_weighted": 0.404679876588068
          },
          {
            "accuracy": 0.4424,
            "f1": 0.43468436125342647,
            "f1_weighted": 0.4346843612534265
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}