{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 8.433661699295044,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.713134328358209,
        "ap": 0.34375305572182413,
        "ap_weighted": 0.34375305572182413,
        "f1": 0.6551912955045766,
        "f1_weighted": 0.7408556336929513,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.713134328358209,
        "scores_per_experiment": [
          {
            "accuracy": 0.6850746268656717,
            "ap": 0.30294640606820167,
            "ap_weighted": 0.30294640606820167,
            "f1": 0.6212711807648517,
            "f1_weighted": 0.715932293525341
          },
          {
            "accuracy": 0.744776119402985,
            "ap": 0.3732214328505221,
            "ap_weighted": 0.3732214328505221,
            "f1": 0.6854365093528894,
            "f1_weighted": 0.7686343131344668
          },
          {
            "accuracy": 0.6850746268656717,
            "ap": 0.34128596187959653,
            "ap_weighted": 0.34128596187959653,
            "f1": 0.6400335089412346,
            "f1_weighted": 0.7175726486592529
          },
          {
            "accuracy": 0.691044776119403,
            "ap": 0.3067115976196528,
            "ap_weighted": 0.3067115976196528,
            "f1": 0.6262319469410526,
            "f1_weighted": 0.721011998212619
          },
          {
            "accuracy": 0.7253731343283583,
            "ap": 0.3609775549732255,
            "ap_weighted": 0.3609775549732255,
            "f1": 0.6702191405581235,
            "f1_weighted": 0.7523462553692757
          },
          {
            "accuracy": 0.6716417910447762,
            "ap": 0.3045428321935222,
            "ap_weighted": 0.3045428321935222,
            "f1": 0.6155090201479534,
            "f1_weighted": 0.7049706237647647
          },
          {
            "accuracy": 0.7761194029850746,
            "ap": 0.3810711350868184,
            "ap_weighted": 0.3810711350868184,
            "f1": 0.703256209474542,
            "f1_weighted": 0.7927989292103772
          },
          {
            "accuracy": 0.753731343283582,
            "ap": 0.37024737460368595,
            "ap_weighted": 0.37024737460368595,
            "f1": 0.6882851028193915,
            "f1_weighted": 0.7752625168564918
          },
          {
            "accuracy": 0.7014925373134329,
            "ap": 0.35095024625939214,
            "ap_weighted": 0.35095024625939214,
            "f1": 0.6532091097308489,
            "f1_weighted": 0.7320076635456259
          },
          {
            "accuracy": 0.6970149253731344,
            "ap": 0.3455760156836245,
            "ap_weighted": 0.3455760156836245,
            "f1": 0.648461226314878,
            "f1_weighted": 0.7280190946512979
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}