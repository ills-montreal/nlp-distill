{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 9.378138780593872,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7697014925373133,
        "ap": 0.4051318120304913,
        "ap_weighted": 0.4051318120304913,
        "f1": 0.7107929752170522,
        "f1_weighted": 0.7900814230408192,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7697014925373133,
        "scores_per_experiment": [
          {
            "accuracy": 0.746268656716418,
            "ap": 0.34794297774809924,
            "ap_weighted": 0.34794297774809924,
            "f1": 0.6736801090979934,
            "f1_weighted": 0.7674020313395037
          },
          {
            "accuracy": 0.7985074626865671,
            "ap": 0.44137030958268286,
            "ap_weighted": 0.44137030958268286,
            "f1": 0.7403928119376254,
            "f1_weighted": 0.8151902173179668
          },
          {
            "accuracy": 0.7134328358208956,
            "ap": 0.35790627425949956,
            "ap_weighted": 0.35790627425949956,
            "f1": 0.6626278520849724,
            "f1_weighted": 0.7423525957936518
          },
          {
            "accuracy": 0.7567164179104477,
            "ap": 0.37080187651537416,
            "ap_weighted": 0.37080187651537416,
            "f1": 0.6899020100573856,
            "f1_weighted": 0.777555638366226
          },
          {
            "accuracy": 0.7865671641791044,
            "ap": 0.43641727854409396,
            "ap_weighted": 0.43641727854409396,
            "f1": 0.7325976349361847,
            "f1_weighted": 0.8057523456707737
          },
          {
            "accuracy": 0.7671641791044777,
            "ap": 0.3969349290578131,
            "ap_weighted": 0.3969349290578131,
            "f1": 0.7068031103780253,
            "f1_weighted": 0.7878140184056323
          },
          {
            "accuracy": 0.808955223880597,
            "ap": 0.44305404646845825,
            "ap_weighted": 0.44305404646845825,
            "f1": 0.7457787896034908,
            "f1_weighted": 0.8229523979419918
          },
          {
            "accuracy": 0.7940298507462686,
            "ap": 0.4405644367250053,
            "ap_weighted": 0.4405644367250053,
            "f1": 0.7379251700680272,
            "f1_weighted": 0.8117661691542287
          },
          {
            "accuracy": 0.7432835820895523,
            "ap": 0.3919341452101359,
            "ap_weighted": 0.3919341452101359,
            "f1": 0.692624481217126,
            "f1_weighted": 0.7686131325257655
          },
          {
            "accuracy": 0.7820895522388059,
            "ap": 0.42439184619375103,
            "ap_weighted": 0.42439184619375103,
            "f1": 0.7255977827896904,
            "f1_weighted": 0.8014156838924508
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}