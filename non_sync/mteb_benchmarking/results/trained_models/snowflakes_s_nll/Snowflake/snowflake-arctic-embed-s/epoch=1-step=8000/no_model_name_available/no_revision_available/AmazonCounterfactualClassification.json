{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 7.737690210342407,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.6364179104477612,
        "ap": 0.2821494336563328,
        "ap_weighted": 0.2821494336563328,
        "f1": 0.5843071258284644,
        "f1_weighted": 0.6734975089982417,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6364179104477612,
        "scores_per_experiment": [
          {
            "accuracy": 0.6014925373134329,
            "ap": 0.25046942347842427,
            "ap_weighted": 0.25046942347842427,
            "f1": 0.5477310707667259,
            "f1_weighted": 0.6426863883037667
          },
          {
            "accuracy": 0.6492537313432836,
            "ap": 0.28072855707582833,
            "ap_weighted": 0.28072855707582833,
            "f1": 0.5909231444746694,
            "f1_weighted": 0.6849898616382842
          },
          {
            "accuracy": 0.5805970149253732,
            "ap": 0.2579439084209327,
            "ap_weighted": 0.2579439084209327,
            "f1": 0.5409067743500833,
            "f1_weighted": 0.62310788173444
          },
          {
            "accuracy": 0.6582089552238806,
            "ap": 0.2780930607403038,
            "ap_weighted": 0.2780930607403038,
            "f1": 0.5936823874304509,
            "f1_weighted": 0.6922847831597367
          },
          {
            "accuracy": 0.655223880597015,
            "ap": 0.28942659476156113,
            "ap_weighted": 0.28942659476156113,
            "f1": 0.5989365092938825,
            "f1_weighted": 0.6904315192208865
          },
          {
            "accuracy": 0.6567164179104478,
            "ap": 0.2773149603045923,
            "ap_weighted": 0.2773149603045923,
            "f1": 0.5924832867902174,
            "f1_weighted": 0.6910062848994428
          },
          {
            "accuracy": 0.6313432835820896,
            "ap": 0.2718281383695794,
            "ap_weighted": 0.2718281383695794,
            "f1": 0.5765558992781825,
            "f1_weighted": 0.6693079855686144
          },
          {
            "accuracy": 0.6626865671641791,
            "ap": 0.31669777279156097,
            "ap_weighted": 0.31669777279156097,
            "f1": 0.6167319705575637,
            "f1_weighted": 0.6975486749347152
          },
          {
            "accuracy": 0.6283582089552239,
            "ap": 0.3064784966054327,
            "ap_weighted": 0.3064784966054327,
            "f1": 0.5924006655281348,
            "f1_weighted": 0.6661226641123176
          },
          {
            "accuracy": 0.6402985074626866,
            "ap": 0.2925134240151128,
            "ap_weighted": 0.2925134240151128,
            "f1": 0.592719549814735,
            "f1_weighted": 0.6774890464102121
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}