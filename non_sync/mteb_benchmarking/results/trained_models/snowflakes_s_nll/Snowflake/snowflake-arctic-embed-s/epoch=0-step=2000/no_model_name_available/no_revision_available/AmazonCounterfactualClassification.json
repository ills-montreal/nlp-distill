{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 8.45418381690979,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.5098507462686567,
        "ap": 0.2018424041473752,
        "ap_weighted": 0.2018424041473752,
        "f1": 0.4586103179633446,
        "f1_weighted": 0.5573820620114208,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5098507462686567,
        "scores_per_experiment": [
          {
            "accuracy": 0.48208955223880595,
            "ap": 0.18928952006916472,
            "ap_weighted": 0.18928952006916472,
            "f1": 0.431980043928766,
            "f1_weighted": 0.5347171262930691
          },
          {
            "accuracy": 0.5835820895522388,
            "ap": 0.20735937485276368,
            "ap_weighted": 0.20735937485276368,
            "f1": 0.5006824780765604,
            "f1_weighted": 0.6245764029193326
          },
          {
            "accuracy": 0.48208955223880595,
            "ap": 0.19798336561467472,
            "ap_weighted": 0.19798336561467472,
            "f1": 0.44228144288596394,
            "f1_weighted": 0.5330172452097605
          },
          {
            "accuracy": 0.4462686567164179,
            "ap": 0.2052335402311377,
            "ap_weighted": 0.2052335402311377,
            "f1": 0.42629969418960245,
            "f1_weighted": 0.4914783878771281
          },
          {
            "accuracy": 0.48656716417910445,
            "ap": 0.19988022633224403,
            "ap_weighted": 0.19988022633224403,
            "f1": 0.44662665066026414,
            "f1_weighted": 0.537158481302969
          },
          {
            "accuracy": 0.5373134328358209,
            "ap": 0.19509461005941298,
            "ap_weighted": 0.19509461005941298,
            "f1": 0.4652199884650243,
            "f1_weighted": 0.5847896035190283
          },
          {
            "accuracy": 0.4955223880597015,
            "ap": 0.19693862603391915,
            "ap_weighted": 0.19693862603391915,
            "f1": 0.4483098001383732,
            "f1_weighted": 0.546589064790934
          },
          {
            "accuracy": 0.5955223880597015,
            "ap": 0.2092653383844138,
            "ap_weighted": 0.2092653383844138,
            "f1": 0.5076882197114512,
            "f1_weighted": 0.6343183988142291
          },
          {
            "accuracy": 0.5238805970149254,
            "ap": 0.21895320413891678,
            "ap_weighted": 0.21895320413891678,
            "f1": 0.4836579043367131,
            "f1_weighted": 0.5714165065437216
          },
          {
            "accuracy": 0.46567164179104475,
            "ap": 0.1984262357571044,
            "ap_weighted": 0.1984262357571044,
            "f1": 0.4333569572407276,
            "f1_weighted": 0.5157594028440363
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}