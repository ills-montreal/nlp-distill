{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 16.34100890159607,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.4429,
        "f1": 0.4366923428546207,
        "f1_weighted": 0.4366923428546207,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4429,
        "scores_per_experiment": [
          {
            "accuracy": 0.456,
            "f1": 0.43618342767233764,
            "f1_weighted": 0.43618342767233764
          },
          {
            "accuracy": 0.463,
            "f1": 0.4567275846763061,
            "f1_weighted": 0.4567275846763061
          },
          {
            "accuracy": 0.4272,
            "f1": 0.422792500099196,
            "f1_weighted": 0.42279250009919606
          },
          {
            "accuracy": 0.4436,
            "f1": 0.4413952654910318,
            "f1_weighted": 0.44139526549103175
          },
          {
            "accuracy": 0.4658,
            "f1": 0.45122405108260005,
            "f1_weighted": 0.4512240510826
          },
          {
            "accuracy": 0.4232,
            "f1": 0.42463566388781826,
            "f1_weighted": 0.4246356638878182
          },
          {
            "accuracy": 0.4154,
            "f1": 0.4133450666787503,
            "f1_weighted": 0.4133450666787504
          },
          {
            "accuracy": 0.4492,
            "f1": 0.4522878428682069,
            "f1_weighted": 0.4522878428682069
          },
          {
            "accuracy": 0.4566,
            "f1": 0.4485063101317667,
            "f1_weighted": 0.4485063101317666
          },
          {
            "accuracy": 0.429,
            "f1": 0.4198257159581935,
            "f1_weighted": 0.41982571595819335
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}