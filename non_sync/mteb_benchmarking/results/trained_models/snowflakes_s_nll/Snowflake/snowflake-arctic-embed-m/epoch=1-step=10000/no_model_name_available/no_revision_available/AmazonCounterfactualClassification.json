{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 8.930384159088135,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7914925373134329,
        "ap": 0.430810217905071,
        "ap_weighted": 0.430810217905071,
        "f1": 0.7320145891480402,
        "f1_weighted": 0.808704900690058,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7914925373134329,
        "scores_per_experiment": [
          {
            "accuracy": 0.7686567164179104,
            "ap": 0.3620214624857723,
            "ap_weighted": 0.3620214624857723,
            "f1": 0.6903059961649831,
            "f1_weighted": 0.785163841456361
          },
          {
            "accuracy": 0.7910447761194029,
            "ap": 0.4462831010872368,
            "ap_weighted": 0.4462831010872368,
            "f1": 0.7386458623572025,
            "f1_weighted": 0.809908385073795
          },
          {
            "accuracy": 0.755223880597015,
            "ap": 0.4054731606510747,
            "ap_weighted": 0.4054731606510747,
            "f1": 0.7043184362352536,
            "f1_weighted": 0.7790285847949608
          },
          {
            "accuracy": 0.7880597014925373,
            "ap": 0.4103640490162883,
            "ap_weighted": 0.4103640490162883,
            "f1": 0.7223195106005417,
            "f1_weighted": 0.8045955777291743
          },
          {
            "accuracy": 0.8149253731343283,
            "ap": 0.4626683161019187,
            "ap_weighted": 0.4626683161019187,
            "f1": 0.7565892017954037,
            "f1_weighted": 0.829153707607237
          },
          {
            "accuracy": 0.8059701492537313,
            "ap": 0.4461351986439162,
            "ap_weighted": 0.4461351986439162,
            "f1": 0.7457854674512001,
            "f1_weighted": 0.8211086274985399
          },
          {
            "accuracy": 0.8343283582089552,
            "ap": 0.4749131871343581,
            "ap_weighted": 0.4749131871343581,
            "f1": 0.7696631823461092,
            "f1_weighted": 0.8439825957321406
          },
          {
            "accuracy": 0.7970149253731343,
            "ap": 0.4465054805309431,
            "ap_weighted": 0.4465054805309431,
            "f1": 0.7417233560090702,
            "f1_weighted": 0.8144941956882253
          },
          {
            "accuracy": 0.7686567164179104,
            "ap": 0.4190481615911717,
            "ap_weighted": 0.4190481615911717,
            "f1": 0.7166678943936529,
            "f1_weighted": 0.7905753486999145
          },
          {
            "accuracy": 0.7910447761194029,
            "ap": 0.4346900618080304,
            "ap_weighted": 0.4346900618080304,
            "f1": 0.7341269841269842,
            "f1_weighted": 0.8090381426202321
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}