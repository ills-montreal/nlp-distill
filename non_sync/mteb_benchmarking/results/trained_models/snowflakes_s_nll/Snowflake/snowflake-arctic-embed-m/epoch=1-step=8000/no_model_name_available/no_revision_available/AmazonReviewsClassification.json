{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 17.51089644432068,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.43938,
        "f1": 0.434247214267642,
        "f1_weighted": 0.434247214267642,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.43938,
        "scores_per_experiment": [
          {
            "accuracy": 0.4572,
            "f1": 0.44340290896024703,
            "f1_weighted": 0.443402908960247
          },
          {
            "accuracy": 0.4538,
            "f1": 0.4469742894821758,
            "f1_weighted": 0.44697428948217577
          },
          {
            "accuracy": 0.4086,
            "f1": 0.40676819888429155,
            "f1_weighted": 0.40676819888429155
          },
          {
            "accuracy": 0.4322,
            "f1": 0.4317381770041421,
            "f1_weighted": 0.43173817700414213
          },
          {
            "accuracy": 0.4576,
            "f1": 0.4415597188932729,
            "f1_weighted": 0.44155971889327295
          },
          {
            "accuracy": 0.415,
            "f1": 0.41779738913658493,
            "f1_weighted": 0.4177973891365848
          },
          {
            "accuracy": 0.4084,
            "f1": 0.4054266660831415,
            "f1_weighted": 0.4054266660831415
          },
          {
            "accuracy": 0.4626,
            "f1": 0.46360443091315967,
            "f1_weighted": 0.4636044309131597
          },
          {
            "accuracy": 0.461,
            "f1": 0.4585151322497434,
            "f1_weighted": 0.45851513224974333
          },
          {
            "accuracy": 0.4374,
            "f1": 0.4266852310696607,
            "f1_weighted": 0.4266852310696608
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}