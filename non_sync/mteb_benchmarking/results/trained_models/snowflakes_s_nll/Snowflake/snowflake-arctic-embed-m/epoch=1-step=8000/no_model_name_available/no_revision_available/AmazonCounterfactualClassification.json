{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 8.10237193107605,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.78,
        "ap": 0.4151557123590389,
        "ap_weighted": 0.4151557123590389,
        "f1": 0.7198800425910663,
        "f1_weighted": 0.7986213137246855,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.78,
        "scores_per_experiment": [
          {
            "accuracy": 0.7671641791044777,
            "ap": 0.3605469063511832,
            "ap_weighted": 0.3605469063511832,
            "f1": 0.6889322746157784,
            "f1_weighted": 0.7839281586377704
          },
          {
            "accuracy": 0.7940298507462686,
            "ap": 0.44984767918067414,
            "ap_weighted": 0.44984767918067414,
            "f1": 0.7415123456790124,
            "f1_weighted": 0.8124631472268288
          },
          {
            "accuracy": 0.735820895522388,
            "ap": 0.38965667843986174,
            "ap_weighted": 0.38965667843986174,
            "f1": 0.6876999770890446,
            "f1_weighted": 0.7623514399057905
          },
          {
            "accuracy": 0.7746268656716417,
            "ap": 0.3795045617479251,
            "ap_weighted": 0.3795045617479251,
            "f1": 0.7018615602686399,
            "f1_weighted": 0.7915541421551195
          },
          {
            "accuracy": 0.7955223880597015,
            "ap": 0.4354011867626999,
            "ap_weighted": 0.4354011867626999,
            "f1": 0.7365467795218865,
            "f1_weighted": 0.8124522946115664
          },
          {
            "accuracy": 0.8,
            "ap": 0.4432188819296836,
            "ap_weighted": 0.4432188819296836,
            "f1": 0.7418453257269048,
            "f1_weighted": 0.8164588700772912
          },
          {
            "accuracy": 0.8253731343283582,
            "ap": 0.4520722868335948,
            "ap_weighted": 0.4520722868335948,
            "f1": 0.7561370162170906,
            "f1_weighted": 0.8352640083442535
          },
          {
            "accuracy": 0.7820895522388059,
            "ap": 0.4313191257610777,
            "ap_weighted": 0.4313191257610777,
            "f1": 0.7283502177197192,
            "f1_weighted": 0.8019262193431668
          },
          {
            "accuracy": 0.7567164179104477,
            "ap": 0.4046690919625201,
            "ap_weighted": 0.4046690919625201,
            "f1": 0.7047891679439043,
            "f1_weighted": 0.7801853173615192
          },
          {
            "accuracy": 0.7686567164179104,
            "ap": 0.40532072462116897,
            "ap_weighted": 0.40532072462116897,
            "f1": 0.7111257611286819,
            "f1_weighted": 0.7896295395835489
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}