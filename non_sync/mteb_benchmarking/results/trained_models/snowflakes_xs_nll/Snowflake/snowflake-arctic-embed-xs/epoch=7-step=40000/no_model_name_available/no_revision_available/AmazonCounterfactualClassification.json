{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 8.284096002578735,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.722089552238806,
        "ap": 0.35486342202816556,
        "ap_weighted": 0.35486342202816556,
        "f1": 0.6649907489050682,
        "f1_weighted": 0.748848038417811,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.722089552238806,
        "scores_per_experiment": [
          {
            "accuracy": 0.6970149253731344,
            "ap": 0.3187204159632096,
            "ap_weighted": 0.3187204159632096,
            "f1": 0.6356305672761369,
            "f1_weighted": 0.7267026331073185
          },
          {
            "accuracy": 0.746268656716418,
            "ap": 0.3857333169599178,
            "ap_weighted": 0.3857333169599178,
            "f1": 0.6916421207658321,
            "f1_weighted": 0.7706762578858286
          },
          {
            "accuracy": 0.682089552238806,
            "ap": 0.3351513425240211,
            "ap_weighted": 0.3351513425240211,
            "f1": 0.6357385158469444,
            "f1_weighted": 0.7148649712774028
          },
          {
            "accuracy": 0.7059701492537314,
            "ap": 0.3166517033154836,
            "ap_weighted": 0.3166517033154836,
            "f1": 0.6387586588172829,
            "f1_weighted": 0.7336454688452101
          },
          {
            "accuracy": 0.7179104477611941,
            "ap": 0.3528369602369831,
            "ap_weighted": 0.3528369602369831,
            "f1": 0.6627328150343978,
            "f1_weighted": 0.7458046753758474
          },
          {
            "accuracy": 0.7074626865671642,
            "ap": 0.34907160005602783,
            "ap_weighted": 0.34907160005602783,
            "f1": 0.655599265670076,
            "f1_weighted": 0.7369849415393528
          },
          {
            "accuracy": 0.7850746268656716,
            "ap": 0.3931063667860075,
            "ap_weighted": 0.3931063667860075,
            "f1": 0.7128605611837954,
            "f1_weighted": 0.800549069511788
          },
          {
            "accuracy": 0.755223880597015,
            "ap": 0.3761387960312131,
            "ap_weighted": 0.3761387960312131,
            "f1": 0.6917673724486932,
            "f1_weighted": 0.7769326860161775
          },
          {
            "accuracy": 0.7253731343283583,
            "ap": 0.38499372171247387,
            "ap_weighted": 0.38499372171247387,
            "f1": 0.680121225959792,
            "f1_weighted": 0.7533862204612802
          },
          {
            "accuracy": 0.6985074626865672,
            "ap": 0.33622999669631815,
            "ap_weighted": 0.33622999669631815,
            "f1": 0.6450563860477314,
            "f1_weighted": 0.7289334601579044
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}