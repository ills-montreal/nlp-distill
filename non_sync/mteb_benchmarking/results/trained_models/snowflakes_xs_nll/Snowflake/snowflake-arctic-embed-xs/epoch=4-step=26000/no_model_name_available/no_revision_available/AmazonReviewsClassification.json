{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 12.32354998588562,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.41150000000000003,
        "f1": 0.4084748762313507,
        "f1_weighted": 0.4084748762313507,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.41150000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.418,
            "f1": 0.41258411893213776,
            "f1_weighted": 0.4125841189321377
          },
          {
            "accuracy": 0.4426,
            "f1": 0.44077971940749733,
            "f1_weighted": 0.4407797194074974
          },
          {
            "accuracy": 0.3934,
            "f1": 0.39294881920955954,
            "f1_weighted": 0.3929488192095596
          },
          {
            "accuracy": 0.4104,
            "f1": 0.4124307880882764,
            "f1_weighted": 0.4124307880882764
          },
          {
            "accuracy": 0.4444,
            "f1": 0.4299985167252383,
            "f1_weighted": 0.42999851672523837
          },
          {
            "accuracy": 0.3784,
            "f1": 0.37840801786868294,
            "f1_weighted": 0.3784080178686829
          },
          {
            "accuracy": 0.3674,
            "f1": 0.3691574000391865,
            "f1_weighted": 0.3691574000391865
          },
          {
            "accuracy": 0.4428,
            "f1": 0.4424045175247896,
            "f1_weighted": 0.4424045175247896
          },
          {
            "accuracy": 0.4206,
            "f1": 0.419566541476882,
            "f1_weighted": 0.41956654147688205
          },
          {
            "accuracy": 0.397,
            "f1": 0.38647032304125617,
            "f1_weighted": 0.38647032304125617
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}