{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 7.651738882064819,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.711044776119403,
        "ap": 0.3435472206645004,
        "ap_weighted": 0.3435472206645004,
        "f1": 0.6541490604253429,
        "f1_weighted": 0.7392101665188595,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.711044776119403,
        "scores_per_experiment": [
          {
            "accuracy": 0.6776119402985075,
            "ap": 0.2983973828183072,
            "ap_weighted": 0.2983973828183072,
            "f1": 0.6151063829787233,
            "f1_weighted": 0.7095592251508415
          },
          {
            "accuracy": 0.7388059701492538,
            "ap": 0.37899863192052097,
            "ap_weighted": 0.37899863192052097,
            "f1": 0.6849566462728154,
            "f1_weighted": 0.7642726179030929
          },
          {
            "accuracy": 0.6716417910447762,
            "ap": 0.3202785632713656,
            "ap_weighted": 0.3202785632713656,
            "f1": 0.623306925632507,
            "f1_weighted": 0.7054761968333645
          },
          {
            "accuracy": 0.7029850746268657,
            "ap": 0.3166490980811516,
            "ap_weighted": 0.3166490980811516,
            "f1": 0.6373699598830489,
            "f1_weighted": 0.7313031767794602
          },
          {
            "accuracy": 0.7164179104477612,
            "ap": 0.36025335334516,
            "ap_weighted": 0.36025335334516,
            "f1": 0.665225532272995,
            "f1_weighted": 0.7449449608810278
          },
          {
            "accuracy": 0.6970149253731344,
            "ap": 0.3351730985204837,
            "ap_weighted": 0.3351730985204837,
            "f1": 0.6437840966531263,
            "f1_weighted": 0.72763806621978
          },
          {
            "accuracy": 0.7716417910447761,
            "ap": 0.3718530669855348,
            "ap_weighted": 0.3718530669855348,
            "f1": 0.6967258462002716,
            "f1_weighted": 0.7885147515953402
          },
          {
            "accuracy": 0.7373134328358208,
            "ap": 0.3600402044541332,
            "ap_weighted": 0.3600402044541332,
            "f1": 0.6757196757196757,
            "f1_weighted": 0.7617821856627828
          },
          {
            "accuracy": 0.7059701492537314,
            "ap": 0.36280887198219836,
            "ap_weighted": 0.36280887198219836,
            "f1": 0.6605850208936033,
            "f1_weighted": 0.7361651530198572
          },
          {
            "accuracy": 0.691044776119403,
            "ap": 0.3310199352661486,
            "ap_weighted": 0.3310199352661486,
            "f1": 0.6387105177466623,
            "f1_weighted": 0.7224453311430473
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}