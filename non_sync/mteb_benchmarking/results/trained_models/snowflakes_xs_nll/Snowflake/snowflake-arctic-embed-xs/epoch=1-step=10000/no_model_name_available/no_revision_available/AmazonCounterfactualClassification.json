{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 7.508982419967651,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.6991044776119403,
        "ap": 0.3260725310142881,
        "ap_weighted": 0.3260725310142881,
        "f1": 0.6395086031527546,
        "f1_weighted": 0.7281790843129006,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6991044776119403,
        "scores_per_experiment": [
          {
            "accuracy": 0.664179104477612,
            "ap": 0.28873678405186154,
            "ap_weighted": 0.28873678405186154,
            "f1": 0.6030084454521754,
            "f1_weighted": 0.6979043727615979
          },
          {
            "accuracy": 0.7373134328358208,
            "ap": 0.3622387922431437,
            "ap_weighted": 0.3622387922431437,
            "f1": 0.6767543859649123,
            "f1_weighted": 0.7619547001832941
          },
          {
            "accuracy": 0.6537313432835821,
            "ap": 0.3094178844370719,
            "ap_weighted": 0.3094178844370719,
            "f1": 0.6083806145380886,
            "f1_weighted": 0.689534550187919
          },
          {
            "accuracy": 0.7194029850746269,
            "ap": 0.33053156092105296,
            "ap_weighted": 0.33053156092105296,
            "f1": 0.6524896265560166,
            "f1_weighted": 0.7453489812349043
          },
          {
            "accuracy": 0.7,
            "ap": 0.3414658880358642,
            "ap_weighted": 0.3414658880358642,
            "f1": 0.6482443561594459,
            "f1_weighted": 0.7304089580541777
          },
          {
            "accuracy": 0.6955223880597015,
            "ap": 0.3136544617441004,
            "ap_weighted": 0.3136544617441004,
            "f1": 0.6322009816584861,
            "f1_weighted": 0.7251331176717806
          },
          {
            "accuracy": 0.7417910447761195,
            "ap": 0.3158180471687365,
            "ap_weighted": 0.3158180471687365,
            "f1": 0.652942250860084,
            "f1_weighted": 0.7598753125643389
          },
          {
            "accuracy": 0.7208955223880597,
            "ap": 0.3401782436418417,
            "ap_weighted": 0.3401782436418417,
            "f1": 0.6581735242039555,
            "f1_weighted": 0.7473392916573162
          },
          {
            "accuracy": 0.6895522388059702,
            "ap": 0.3402753344160296,
            "ap_weighted": 0.3402753344160296,
            "f1": 0.642082986614068,
            "f1_weighted": 0.7214578017546257
          },
          {
            "accuracy": 0.6686567164179105,
            "ap": 0.31840831348317883,
            "ap_weighted": 0.31840831348317883,
            "f1": 0.6208088595203133,
            "f1_weighted": 0.7028337570590513
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}