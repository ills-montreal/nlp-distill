{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 7.628296375274658,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.6411940298507462,
        "ap": 0.2717883778397768,
        "ap_weighted": 0.2717883778397768,
        "f1": 0.5803877421647117,
        "f1_weighted": 0.6771639380694724,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6411940298507462,
        "scores_per_experiment": [
          {
            "accuracy": 0.6373134328358209,
            "ap": 0.2642335666155175,
            "ap_weighted": 0.2642335666155175,
            "f1": 0.5747431768494793,
            "f1_weighted": 0.674076501528185
          },
          {
            "accuracy": 0.6791044776119403,
            "ap": 0.30516612346968564,
            "ap_weighted": 0.30516612346968564,
            "f1": 0.6195946392024824,
            "f1_weighted": 0.7112173338102517
          },
          {
            "accuracy": 0.6014925373134329,
            "ap": 0.26011735217044546,
            "ap_weighted": 0.26011735217044546,
            "f1": 0.5538224019115127,
            "f1_weighted": 0.6426325171808434
          },
          {
            "accuracy": 0.6298507462686567,
            "ap": 0.252491106349916,
            "ap_weighted": 0.252491106349916,
            "f1": 0.5630312210721198,
            "f1_weighted": 0.6670860542026047
          },
          {
            "accuracy": 0.6537313432835821,
            "ap": 0.28123695180342345,
            "ap_weighted": 0.28123695180342345,
            "f1": 0.5934466019417475,
            "f1_weighted": 0.6887806115055788
          },
          {
            "accuracy": 0.6761194029850747,
            "ap": 0.29750779604943894,
            "ap_weighted": 0.29750779604943894,
            "f1": 0.6138779781111651,
            "f1_weighted": 0.708281328849288
          },
          {
            "accuracy": 0.6686567164179105,
            "ap": 0.25997641229478147,
            "ap_weighted": 0.25997641229478147,
            "f1": 0.5855670103092784,
            "f1_weighted": 0.698569010617018
          },
          {
            "accuracy": 0.6611940298507463,
            "ap": 0.2741742363822801,
            "ap_weighted": 0.2741742363822801,
            "f1": 0.5925524077422811,
            "f1_weighted": 0.6943916143613859
          },
          {
            "accuracy": 0.6373134328358209,
            "ap": 0.2909349319170443,
            "ap_weighted": 0.2909349319170443,
            "f1": 0.5902821292947901,
            "f1_weighted": 0.6748141638531187
          },
          {
            "accuracy": 0.5671641791044776,
            "ap": 0.2320453013452342,
            "ap_weighted": 0.2320453013452342,
            "f1": 0.5169598552122592,
            "f1_weighted": 0.6117902447864494
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}