{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "evaluation_time": 6.699265956878662,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.83",
  "scores": {
    "test": [
      {
        "accuracy": 0.7062686567164179,
        "ap": 0.33884207669676825,
        "ap_weighted": 0.33884207669676825,
        "f1": 0.6494365980184017,
        "f1_weighted": 0.7350108091144245,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7062686567164179,
        "scores_per_experiment": [
          {
            "accuracy": 0.664179104477612,
            "ap": 0.29440963123314723,
            "ap_weighted": 0.29440963123314723,
            "f1": 0.6062436822680364,
            "f1_weighted": 0.6982189828964677
          },
          {
            "accuracy": 0.7388059701492538,
            "ap": 0.37678285028176794,
            "ap_weighted": 0.37678285028176794,
            "f1": 0.6840125155298753,
            "f1_weighted": 0.7641405781990738
          },
          {
            "accuracy": 0.6776119402985075,
            "ap": 0.33013622511570373,
            "ap_weighted": 0.33013622511570373,
            "f1": 0.6310572687224669,
            "f1_weighted": 0.7108652771385364
          },
          {
            "accuracy": 0.6985074626865672,
            "ap": 0.3035090794443888,
            "ap_weighted": 0.3035090794443888,
            "f1": 0.6278146278146278,
            "f1_weighted": 0.7265909176356938
          },
          {
            "accuracy": 0.7059701492537314,
            "ap": 0.3500665423895627,
            "ap_weighted": 0.3500665423895627,
            "f1": 0.6552444684746808,
            "f1_weighted": 0.7357739539137962
          },
          {
            "accuracy": 0.6850746268656717,
            "ap": 0.3269821376577672,
            "ap_weighted": 0.3269821376577672,
            "f1": 0.63366062104333,
            "f1_weighted": 0.717233985089208
          },
          {
            "accuracy": 0.7626865671641792,
            "ap": 0.3607193042421481,
            "ap_weighted": 0.3607193042421481,
            "f1": 0.687282590508397,
            "f1_weighted": 0.7807926892183031
          },
          {
            "accuracy": 0.7283582089552239,
            "ap": 0.350395660153053,
            "ap_weighted": 0.350395660153053,
            "f1": 0.6667905431254031,
            "f1_weighted": 0.7540114030509826
          },
          {
            "accuracy": 0.7119402985074627,
            "ap": 0.3674631580188031,
            "ap_weighted": 0.3674631580188031,
            "f1": 0.6657784072845885,
            "f1_weighted": 0.7414171688064064
          },
          {
            "accuracy": 0.6895522388059702,
            "ap": 0.3279561784313401,
            "ap_weighted": 0.3279561784313401,
            "f1": 0.6364812554126104,
            "f1_weighted": 0.7210631351957774
          }
        ]
      }
    ]
  },
  "task_name": "AmazonCounterfactualClassification"
}